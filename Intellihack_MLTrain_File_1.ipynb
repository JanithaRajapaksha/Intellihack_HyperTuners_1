{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INTELLIHACK 5.0 - TEAM HYPER TUNERS\n",
        "## Task 1 - Part 1: Weather Prediction Model - Training file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pxrEAfeg29wy",
        "outputId": "cd5bacbe-e32f-4db8-e3a4-79c36632db21"
      },
      "outputs": [],
      "source": [
        "# --- Install Required Libraries ---\n",
        "!pip install lightgbm optuna\n",
        "\n",
        "# --- Import Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
        "from lightgbm import LGBMClassifier\n",
        "import optuna\n",
        "from datetime import timedelta\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Step 1: Load the Dataset ---\n",
        "np.random.seed(42)\n",
        "file_path = '/content/weather_data.csv'  # Update this path\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(\"Dataset Loaded Successfully. Shape:\", df.shape)\n",
        "    print(\"First 5 Rows:\\n\", df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: File not found. Please provide the correct path to 'weather_data.csv'.\")\n",
        "    exit()\n",
        "\n",
        "# --- Step 2: Preprocessing ---\n",
        "numeric_cols = ['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure']\n",
        "df[numeric_cols] = df[numeric_cols].clip(lower=0)\n",
        "df['humidity'] = df['humidity'].clip(upper=100)\n",
        "df['cloud_cover'] = df['cloud_cover'].clip(upper=100)\n",
        "df['rain_or_not'] = df['rain_or_not'].map({'Rain': 1, 'No Rain': 0})\n",
        "\n",
        "print(\"\\nMissing Values Before Preprocessing:\\n\", df.isnull().sum())\n",
        "df = df.infer_objects(copy=False)\n",
        "df.interpolate(method='linear', inplace=True)\n",
        "df.ffill(inplace=True)\n",
        "df.bfill(inplace=True)\n",
        "print(\"\\nAfter Interpolation and Fill:\\n\", df.isnull().sum())\n",
        "\n",
        "if df[numeric_cols].isnull().sum().sum() > 0:\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(df[numeric_cols])\n",
        "    imputer = KNNImputer(n_neighbors=4)\n",
        "    X_imputed = imputer.fit_transform(X_scaled)\n",
        "    df[numeric_cols] = scaler.inverse_transform(X_imputed)\n",
        "    print(\"\\nAfter KNN Imputation:\\n\", df.isnull().sum())\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# --- Step 3: Feature Engineering ---\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['temp_humidity_interaction'] = df['avg_temperature'] * df['humidity']\n",
        "df['cloud_pressure_ratio'] = df['cloud_cover'] / (df['pressure'] + 1e-6)\n",
        "df['month'] = df['date'].dt.month\n",
        "\n",
        "features = [\n",
        "    'avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure',\n",
        "    'temp_humidity_interaction', 'cloud_pressure_ratio', 'month'\n",
        "]\n",
        "X = df[features]\n",
        "y = df['rain_or_not']\n",
        "\n",
        "# --- Step 4: Train-Test Split and Scaling ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100, stratify=y)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 5: Define Optuna Objective Function ---\n",
        "def objective(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
        "        'num_leaves': trial.suggest_int('num_leaves', 20, 50),\n",
        "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 30),\n",
        "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n",
        "        'reg_lambda': trial.suggest_float('reg_lambda', 0, 1),\n",
        "    }\n",
        "\n",
        "    lgb_model = LGBMClassifier(**params, random_state=100, verbose=-1)\n",
        "    lgb_model.fit(X_train_scaled, y_train)\n",
        "    y_pred = lgb_model.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    return accuracy\n",
        "\n",
        "# --- Step 6: Run Optuna Optimization ---\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(\"\\nBest Parameters Found by Optuna for LightGBM:\")\n",
        "print(study.best_params)\n",
        "print(\"Best Test Accuracy:\", study.best_value)\n",
        "\n",
        "# --- Step 7: Train Final Model with Best Parameters ---\n",
        "best_lgb_model = LGBMClassifier(\n",
        "    n_estimators=study.best_params['n_estimators'],\n",
        "    max_depth=study.best_params['max_depth'],\n",
        "    learning_rate=study.best_params['learning_rate'],\n",
        "    num_leaves=study.best_params['num_leaves'],\n",
        "    min_child_samples=study.best_params['min_child_samples'],\n",
        "    subsample=study.best_params['subsample'],\n",
        "    colsample_bytree=study.best_params['colsample_bytree'],\n",
        "    reg_alpha=study.best_params['reg_alpha'],\n",
        "    reg_lambda=study.best_params['reg_lambda'],\n",
        "    random_state=100,\n",
        "    verbose=-1\n",
        ")\n",
        "best_lgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "train_pred = best_lgb_model.predict(X_train_scaled)\n",
        "test_pred = best_lgb_model.predict(X_test_scaled)\n",
        "print(\"\\nTuned LightGBM Results:\")\n",
        "print(\"Train Accuracy:\", accuracy_score(y_train, train_pred))\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, test_pred))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, best_lgb_model.predict_proba(X_test_scaled)[:, 1]))\n",
        "\n",
        "# --- Step 8: Feature Importance ---\n",
        "feature_importance = pd.Series(best_lgb_model.feature_importances_, index=features).sort_values(ascending=False)\n",
        "plt.figure(figsize=(10, 6))\n",
        "feature_importance.plot(kind='bar')\n",
        "plt.title(\"Feature Importance in LightGBM Model\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.ylabel(\"Importance\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Step 9: Probability Output for Next 21 Days ---\n",
        "last_date = df['date'].max()\n",
        "future_dates = [last_date + timedelta(days=i) for i in range(1, 22)]\n",
        "future_data = pd.DataFrame({\n",
        "    col: df[col].tail(21).mean() + np.random.normal(0, df[col].std() / 10, 21)\n",
        "    for col in features\n",
        "})\n",
        "future_data_scaled = scaler.transform(future_data)\n",
        "\n",
        "future_probabilities = best_lgb_model.predict_proba(future_data_scaled)[:, 1]\n",
        "future_predictions = pd.DataFrame({\n",
        "    'Date': future_dates,\n",
        "    'Rain_Probability': future_probabilities\n",
        "})\n",
        "future_predictions['Rain_or_No_Rain'] = future_predictions['Rain_Probability'].apply(lambda x: 'Rain' if x >= 0.5 else 'No Rain')\n",
        "\n",
        "print(\"\\nRain Probabilities for Next 21 Days:\\n\", future_predictions)\n",
        "\n",
        "# Visualize predictions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(future_predictions['Date'], future_predictions['Rain_Probability'], marker='o')\n",
        "plt.axhline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
        "plt.title(\"Rain Probability for Next 21 Days (LightGBM Model)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Probability of Rain\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
