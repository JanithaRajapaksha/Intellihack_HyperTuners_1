{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTELLIHACK 5.0 - TEAM HYPER TUNERS\n",
    "## Task 1 - Part 1: Weather Prediction Model\n",
    "  \n",
    "✅ **Objective**: Build a machine learning model to predict the probability of rain over the next 21 days using historical weather data, with thorough preprocessing, analysis, and optimization.  \n",
    "**Dataset**: `weather_data.csv` (assumed columns: `date`, `avg_temperature`, `humidity`, `avg_wind_speed`, `cloud_cover`, `pressure`, `rain_or_not`)  \n",
    "**Approach**:  \n",
    "- Preprocess data to handle missing values, outliers, and feature engineering.  \n",
    "- Perform exploratory data analysis (EDA) to uncover patterns.  \n",
    "- Train and tune LightGBM and ensemble models for prediction.  \n",
    "- Generate and visualize rain probabilities for the next 21 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Initial Setup](#initial-setup)  \n",
    "2. [Loading the Dataset](#step-1-loading-the-dataset)  \n",
    "3. [Data Preprocessing](#step-2-data-preprocessing)  \n",
    "4. [Feature Engineering](#step-3-feature-engineering)  \n",
    "5. [Exploratory Data Analysis](#step-4-exploratory-data-analysis)  \n",
    "6. [Model Training and Evaluation](#step-5-model-training-and-evaluation)  \n",
    "7. [Hyperparameter Tuning](#step-6-hyperparameter-tuning)  \n",
    "8. [Ensemble Model](#step-7-ensemble-model)  \n",
    "9. [Feature Importance Analysis](#step-8-feature-importance-analysis)  \n",
    "10. [Rain Probability Prediction](#step-9-rain-probability-prediction)  \n",
    "11. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import lightgbm as lgb\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    df = pd.read_csv('weather_data.csv') \n",
    "    print(\"Dataset Loaded Successfully. Shape:\", df.shape)\n",
    "    print(\"First 5 Rows:\\n\", df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File not found. Please provide the correct path to 'weather_data.csv'.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **Preprocessing Notes:**  \n",
    "- Outliers (e.g., negative values) are clipped to realistic ranges (e.g., humidity ≤ 100%).  \n",
    "- Missing values are filled using linear interpolation for time-series continuity, with forward/backward fill and KNN imputation as backups.  \n",
    "- The target `rain_or_not` is encoded as binary (1 = Rain, 0 = No Rain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - --- Preprocessing ---\n",
    "\n",
    "# Define numeric columns for outlier checks and imputation\n",
    "numeric_cols = ['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure']\n",
    "\n",
    "# Handle outliers and incorrect entries\n",
    "df[numeric_cols] = df[numeric_cols].clip(lower=0)  # Ensure no negative values\n",
    "df['humidity'] = df['humidity'].clip(upper=100)    # Cap humidity at 100%\n",
    "df['cloud_cover'] = df['cloud_cover'].clip(upper=100)  # Cap cloud cover at 100%\n",
    "\n",
    "# Encode target variable\n",
    "df['rain_or_not'] = df['rain_or_not'].map({'Rain': 1, 'No Rain': 0})\n",
    "\n",
    "# Check missing values before preprocessing\n",
    "print(\"\\nMissing Values Before Preprocessing:\\n\", df.isnull().sum())\n",
    "\n",
    "# Clip outliers (e.g., negative values to 0, cap humidity/cloud_cover at 100)\n",
    "df['humidity'] = df['humidity'].clip(0, 100)\n",
    "df['cloud_cover'] = df['cloud_cover'].clip(0, 100)\n",
    "\n",
    "# Step 1.1: Time-series interpolation for temporal continuity\n",
    "df = df.infer_objects(copy=False)  # Address FutureWarning for object dtype\n",
    "df.interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Step 1.2: Forward and backward fill as fallback\n",
    "df.ffill(inplace=True)  # Replace deprecated fillna(method='ffill')\n",
    "df.bfill(inplace=True)  # Replace deprecated fillna(method='bfill')\n",
    "print(\"\\nAfter Interpolation and Fill:\\n\", df.isnull().sum())\n",
    "\n",
    "# Conditional KNN imputation if NaNs persist\n",
    "if df[numeric_cols].isnull().sum().sum() > 0:\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(df[numeric_cols])\n",
    "    imputer = KNNImputer(n_neighbors=4)\n",
    "    X_imputed = imputer.fit_transform(X_scaled)\n",
    "    df[numeric_cols] = scaler.inverse_transform(X_imputed)\n",
    "    print(\"\\nAfter KNN Imputation:\\n\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index after preprocessing\n",
    "df.reset_index(drop=True, inplace=True)  # Drop old index to avoid duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features\n",
    "features = ['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure',\n",
    "            'temp_humidity_interaction', 'cloud_pressure_ratio', 'month']\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary Statistics:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'numeric_cols' is defined\n",
    "numeric_cols = ['avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure', 'rain_or_not']\n",
    "\n",
    "# --- Feature Engineering ---\n",
    "\n",
    "# Convert 'date' column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Add engineered features\n",
    "df['temp_humidity_interaction'] = df['avg_temperature'] * df['humidity']  # Interaction term\n",
    "df['cloud_pressure_ratio'] = df['cloud_cover'] / (df['pressure'] + 1e-6)  # Avoid division by zero\n",
    "df['month'] = df['date'].dt.month  # Extract month for seasonality\n",
    "\n",
    "# Define final feature set and target\n",
    "features = [\n",
    "    'avg_temperature', 'humidity', 'avg_wind_speed', 'cloud_cover', 'pressure',\n",
    "    'temp_humidity_interaction', 'cloud_pressure_ratio', 'month'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✅ **EDA Insights:**  \n",
    "- The correlation matrix highlights strong relationships between `humidity` and `rain_or_not`.  \n",
    "- Boxplots reveal distinct feature distributions for rainy vs. non-rainy days.  \n",
    "- Engineered features like `temp_humidity_interaction` may capture combined effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 3 - --- Exploratory Data Analysis (EDA) ---\n",
    "\n",
    "# 1. Correlation Matrix\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df[numeric_cols + ['rain_or_not']].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title(\"Correlation Matrix of Features and Target\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Feature vs Target Analysis\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, col in enumerate(features[:6], 1):  # Plot first 6 features\n",
    "    plt.subplot(2, 3, i)\n",
    "    sns.boxplot(x='rain_or_not', y=col, data=df, color='lightgreen')\n",
    "    plt.title(f'{col} vs Rain')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Distribution of Features\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(features + ['temp_humidity_interaction', 'cloud_pressure_ratio', 'month'], 1):  # Plot all features\n",
    "    plt.subplot(4, 3, i)\n",
    "    sns.histplot(df[col], bins=20, kde=True, color='royalblue')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Pairplot for Key Features\n",
    "sns.pairplot(df[['avg_temperature', 'humidity', 'cloud_cover', 'rain_or_not']], hue='rain_or_not')\n",
    "plt.show()\n",
    "\n",
    "# 5. Histograms\n",
    "# Plot histograms for numeric features\n",
    "numeric_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "for feature in numeric_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(df[feature], kde=True, color='gray')\n",
    "    plt.title(f'Histogram of {feature}')\n",
    "    plt.show()\n",
    "\n",
    "# 6. Scatter Plots\n",
    "# Scatter plot between avg_temperature and humidity\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='avg_temperature', y='humidity', hue='rain_or_not', data=df, color='darkblue')\n",
    "plt.xlabel('Avg Temperature')\n",
    "plt.ylabel('Humidity')\n",
    "plt.title('Scatter Plot of Avg Temperature vs Humidity')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Summary statistics\n",
    "print(\"Summary Statistics:\\n\", df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 -  --- Model Training and Evaluation ---\n",
    "\n",
    "# Define features and target variable\n",
    "X = df[features]\n",
    "y = df['rain_or_not']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=100, stratify=y)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(random_state=100)\n",
    "lgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate LightGBM\n",
    "train_pred = lgb_model.predict(X_train_scaled)\n",
    "test_pred = lgb_model.predict(X_test_scaled)\n",
    "train_prob = lgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "test_prob = lgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"\\n--- LightGBM Results: ---\")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, train_pred)*100, \"%\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_pred)*100, \"%\")\n",
    "print(\"Test Classification Report:\\n\", classification_report(y_test, test_pred))\n",
    "print(\"Test ROC AUC:\", roc_auc_score(y_test, test_prob)*100)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 - --- Hyperparameter Tuning for LightGBM ---\n",
    "\n",
    "# Define parameter grid for LightGBM - Took the best pre-trained values from the previous run\n",
    "param_grid = {\n",
    "    'learning_rate': [0.11197348326497208],\n",
    "    'n_estimators': [153],\n",
    "    'num_leaves': [27],\n",
    "    'max_depth': [8],\n",
    "    'min_child_samples': [30],\n",
    "    'subsample': [0.9583523180973899],\n",
    "    'colsample_bytree': [0.8034866277716478], \n",
    "    'reg_alpha': [0.3105330288217051],\n",
    "    'reg_lambda': [0.10291039503067402]\n",
    "}\n",
    "\n",
    "# Initialize LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(random_state=100)\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=lgb_model, param_grid=param_grid, cv=3, scoring='accuracy')\n",
    "grid_search.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Post-Tuning Analysis and Prediction ---\n",
    "\n",
    "# Extract the best LightGBM model\n",
    "best_lgb_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the tuned model on test data\n",
    "test_pred_tuned = best_lgb_model.predict(X_test_scaled)\n",
    "test_prob_tuned = best_lgb_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensemble Method: Combine Tuned LightGBM with Random Forest ---\n",
    "\n",
    "# Initialize Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=150,           # Fewer trees for small dataset efficiency\n",
    "    max_depth=7,              # Moderate depth to balance complexity\n",
    "    min_samples_split=5,      # Prevent over-splitting on small data\n",
    "    min_samples_leaf=2,       # Ensure robust leaves\n",
    "    max_features='sqrt',      # Use sqrt(n_features) for diversity\n",
    "    n_jobs=-5,                # Utilize all CPU cores for speed\n",
    "    random_state=100,\n",
    "    bootstrap=True         # Consistent with LightGBM for reproducibility\n",
    ")\n",
    "\n",
    "# Create ensemble with soft voting\n",
    "ensemble_model = VotingClassifier(\n",
    "    estimators=[('lgb', best_lgb_model), ('rf', rf_model)],\n",
    "    voting='soft'  # Average probabilities\n",
    ")\n",
    "\n",
    "# Fit ensemble on training data\n",
    "ensemble_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate ensemble\n",
    "train_pred_ensemble = ensemble_model.predict(X_train_scaled)\n",
    "test_pred_ensemble = ensemble_model.predict(X_test_scaled)\n",
    "test_prob_ensemble = ensemble_model.predict_proba(X_test_scaled)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Results ---\n",
    "\n",
    "# Display best parameters and score from GridSearchCV\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "\n",
    "print(\"\\n --- Ensemble (LightGBM + Random Forest) Results: ---\")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, train_pred_ensemble)*100,\"%\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_pred_ensemble)*100,\"%\")\n",
    "print(\"roc_auc_score:\", roc_auc_score(y_test, test_prob_ensemble)*100)\n",
    "#print(\"Test Classification Report:\\n\", classification_report(y_test, test_pred_ensemble))\n",
    "#print(\"Test ROC AUC:\", roc_auc_score(y_test, test_prob_ensemble))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred_ensemble))\n",
    "\n",
    "\n",
    "print(\"\\n --- Tuned LightGBM Results: --- \")\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, best_lgb_model.predict(X_train_scaled))*100,\"%\")\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, test_pred_tuned)*100,\"%\")\n",
    "print(\"roc_auc_score:\", roc_auc_score(y_test, test_prob_tuned)*100)\n",
    "#print(\"Test Classification Report:\\n\", classification_report(y_test, test_pred_tuned))\n",
    "#print(\"Test ROC AUC:\", roc_auc_score(y_test, test_prob_tuned))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, test_pred_tuned))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💭  Assumptions and Limitations\n",
    "- **Assumptions**: Historical data is representative of future weather patterns; simulated future data uses historical averages with minor noise.  \n",
    "- **Limitations**: The model excludes external factors (e.g., geographical influences, extreme weather events) and assumes stationarity in weather trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Graph\n",
    "\n",
    "# Ensure the model is fitted\n",
    "if not hasattr(lgb_model, 'feature_importances_'):\n",
    "\tlgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({'Feature': features, 'Importance': lgb_model.feature_importances_})\n",
    "feature_importance = feature_importance.sort_values('Importance', ascending=False)  \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance, color='red')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Step 7 - --- Probability Output for Next 21 Days ---\n",
    "\n",
    "# Generate future data for next 21 days\n",
    "last_date = df['date'].max()\n",
    "future_dates = [last_date + timedelta(days=i) for i in range(1, 22)]\n",
    "future_data = pd.DataFrame({\n",
    "    col: df[col].tail(21).mean() + np.random.normal(0, df[col].std() / 10, 21)\n",
    "    for col in features\n",
    "})\n",
    "future_data_scaled = scaler.transform(future_data)\n",
    "\n",
    "# Predict Probabilities\n",
    "future_probabilities = best_lgb_model.predict_proba(future_data_scaled)[:, 1] #change to ensemble_model for ensemble model\n",
    "future_predictions = pd.DataFrame({\n",
    "    'Date': future_dates,\n",
    "    'Rain_Probability': future_probabilities\n",
    "})\n",
    "\n",
    "# Add a column for Rain or No Rain based on the threshold\n",
    "future_predictions['Rain_or_No_Rain'] = future_predictions['Rain_Probability'].apply(lambda x: 'Rain' if x >= 0.5 else 'No Rain')\n",
    "\n",
    "print(\"Rain Probabilities for Next 21 Days:\\n\", future_predictions)\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(future_predictions['Date'], future_predictions['Rain_Probability'], marker='o')\n",
    "plt.axhline(0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "plt.title(\"Rain Probability for Next 21 Days (Ensemble Model)\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Probability of Rain\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Contextual explanation\n",
    "print(\"🔴 The Probability of rain is predicted for each day. A probability above 0.5 indicates rain,\\nwhile below 0.5 indicates no rain. The shaded area represents uncertainty in the prediction.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
